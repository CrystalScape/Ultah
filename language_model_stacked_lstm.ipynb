{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0gTyfL_Huko"
   },
   "source": [
    "# LSTM Based Language Model - Part II\n",
    "A language model looks at the context to generate next set of words. This context is also called as a sliding window which moves across the input sentence from left to right(right to left for language which are written from right to left). \n",
    "\n",
    "This is the second notebook with same layout. We present two variants of the model, one with stacked LSTM layers and one with a bidirectional LSTM layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_9/language_model_stacked_lstm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laf8nuPYIRQO"
   },
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "lqviC-HVpmm2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "GM1hw8f4p67G",
    "outputId": "55998598-2d64-4189-edb7-97fa21edcec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version=2.10.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version={}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ3sFTVvIhwD"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nr9VOpsop81u"
   },
   "outputs": [],
   "source": [
    "# https://www.gutenberg.org/ebooks/2600\n",
    "datafile_path = r'gabungan.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "g_6iVpkqqEAN",
    "outputId": "3a0a2a76-0d56-4b63-f9b3-3cebbd0cb784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book contains a total of 77309 characters\n"
     ]
    }
   ],
   "source": [
    "# Load the text file\n",
    "text = open(datafile_path, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Book contains a total of {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "WUcBctq8qFDq",
    "outputId": "6d0066da-e20d-4898-addc-f087ca27b606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ah-mudahan ada mawar yang bisa kupetik dan kupersembahkan kepadamu.\n",
      "Sayangku, tunggulah aku.\n",
      "\n",
      "Sejak aku melihatmu tuk pertama kali, aku merasakan getaran yang berbeda. Betapa aku tak menyadarinya. Namun, setelah kesekian kali aku melihatmu, aku sadar, selama ini aku menyimpan rasa sama kamu. Rasanya seperti ada yang hilang jika sehari saja ga melihat kamu.\n",
      "\n",
      "Kamu harus tahu, betapa senangnya aku, jika tatap mata ini berbalas. Bukan hanya itu, aku juga suka tawamu, candamu dan senyumanmu itu.\n"
     ]
    }
   ],
   "source": [
    "idx = 8091\n",
    "print(text[idx:idx+500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "NK5VucWVqNiG"
   },
   "outputs": [],
   "source": [
    "# We remove first 8k characters to remove \n",
    "# details related to project gutenberg\n",
    "text = text [8091:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BgqxMDT1qN1j",
    "outputId": "7f279d68-0c22-4101-c2de-6127bb3dd8b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7K7kUR6Iup6"
   },
   "source": [
    "## Prepare Dataset\n",
    "+ Dictionary of character to index mapping\n",
    "+ Inverse mapping of index to character mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "G-1s-z3RqOG7"
   },
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "14v7MGryqOV2",
    "outputId": "71a1a5df-f323-47ca-b13b-6350cfaeb1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  \"'\" :   3,\n",
      "  ',' :   4,\n",
      "  '-' :   5,\n",
      "  '.' :   6,\n",
      "  '1' :   7,\n",
      "  '2' :   8,\n",
      "  '3' :   9,\n",
      "  '5' :  10,\n",
      "  '6' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'H' :  18,\n",
      "  'I' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6kEpFB1I9NY"
   },
   "source": [
    "### Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Dp_GTAFiqVq1",
    "outputId": "20f3539b-f4a6-4a0c-8f39-5124eb98eefa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'an kupersembahkan ke' ---- char-2-int ----  [33 46  2 43 52 48 37 49 50 37 45 34 33 40 43 33 46  2 43 37]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- char-2-int ----  {}'.format(repr(text[40:60]), text_as_int[40:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Emy0LrJtJFbw"
   },
   "source": [
    "### Prepare Batch of Training Samples\n",
    "+ Sequence length limit to 100\n",
    "+ Use ``tf.data`` API to prepare batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "0XdE0xYcqWJC",
    "outputId": "bb1f6943-e514-4bde-b748-b776cc8cc010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "h\n",
      "-\n",
      "m\n",
      "u\n",
      "d\n",
      "a\n",
      "h\n",
      "a\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "ws-2k5p2qWWG",
    "outputId": "8718f678-274e-496b-dcbb-681c3de174ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ah-mudahan ada mawar yang bisa kupetik dan kupersembahkan kepadamu.\\r\\nSayangku, tunggulah aku.\\r\\n\\r\\nSeja'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'k aku melihatmu tuk pertama kali, aku merasakan getaran yang berbeda. Betapa aku tak menyadarinya. Na'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'mun, setelah kesekian kali aku melihatmu, aku sadar, selama ini aku menyimpan rasa sama kamu. Rasanya'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "' seperti ada yang hilang jika sehari saja ga melihat kamu.\\r\\n\\r\\nKamu harus tahu, betapa senangnya aku, '\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'jika tatap mata ini berbalas. Bukan hanya itu, aku juga suka tawamu, candamu dan senyumanmu itu. Kala'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'u boleh, betapa aku menyukai semua yang ada pada dirimu. Setiap saat yang ada dipikiranku cuma kamu s'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'eorang. Bukan pak Tatang guru matematika ituâ€¦ suer, hehe\\r\\n\\r\\nNadia yang manis,\\r\\nSungguh tak tahan rasa'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'nya menyimpan rasa ini terus menerus. Memendam perasan ternyata lebih sakit dari apapun. Aku tidak bi'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'sa menahan semua ini. Karena itu, dalam surat ini aku ingin bilang, AKU MENYUKAIMU. Ini sebuah kejuju'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'ran, bukan rayuan gombal seperti kata Judika, jebolan Indonesian Idol itu. Maukah kamu jadi Cewek-ku?'\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(10):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print(\"-\"*110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfJ8lzfSJuBU"
   },
   "source": [
    "### Prepare Input->Target samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "L-7b_73PqWji"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    \"\"\"\n",
    "    Utility which takes a chunk of input text and target as one position shifted form of input chunk.\n",
    "    Parameters:\n",
    "        chunk: input list of words\n",
    "    Returns:\n",
    "        Tuple-> input_text(i.e. chunk minus last word),target_text(input chunk minus the first word)\n",
    "    \"\"\"\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "ZNpd4pilqWw2",
    "outputId": "4ccace3f-935c-42b0-de0e-4a8bbd6d93f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'ah-mudahan ada mawar yang bisa kupetik dan kupersembahkan kepadamu.\\r\\nSayangku, tunggulah aku.\\r\\n\\r\\nSej'\n",
      "Target data: 'h-mudahan ada mawar yang bisa kupetik dan kupersembahkan kepadamu.\\r\\nSayangku, tunggulah aku.\\r\\n\\r\\nSeja'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "MW75dGqMqlAj"
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "duuMnJEDqlNX",
    "outputId": "d8e73f32-e065-407e-e658-d7a2aebdb5fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape=<BatchDataset element_spec=(TensorSpec(shape=(128, 100), dtype=tf.int32, name=None), TensorSpec(shape=(128, 100), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(\"Dataset Shape={}\".format(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL12LqyWJ0tn"
   },
   "source": [
    "## Prepare Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "j_sIzibsqlof"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size,is_bidirectional=False):\n",
    "    \"\"\"\n",
    "    Utility to create a model object.\n",
    "    Parameters:\n",
    "        vocab_size: number of unique characters\n",
    "        embedding_dim: size of embedding vector. This typically in powers of 2, i.e. 64, 128, 256 and so on\n",
    "        rnn_units: number of LSTM units to be used\n",
    "        batch_size: batch size for training the model\n",
    "    Returns:\n",
    "        tf.keras model object\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]))\n",
    "    if is_bidirectional:\n",
    "      model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform')))\n",
    "    else:\n",
    "      model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipJBSRk_KEGK"
   },
   "source": [
    "### Define the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "6BeShRs8qwCF"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "I-LCpWVEqwU0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.bias\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "iujYNi4cqv5N",
    "outputId": "f1273616-7886-41cc-9330-fd6f2128a55d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (128, None, 256)          16128     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (128, None, 1024)         5246976   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (128, None, 1024)         8392704   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (128, None, 63)           64575     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,720,383\n",
      "Trainable params: 13,720,383\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "IAUK1SDWql6N"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "xM5dHsLwrx-S"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAMJXmvYKSGh"
   },
   "source": [
    "### Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "4BpfQbQXqmec"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = r'data/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "m6-WTUO9q7is",
    "outputId": "c5ae9d16-edd9-4142-f1ea-bcc8f99646d2"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "#history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IH2kBWPjKZ_p"
   },
   "source": [
    "## Generate Fake Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5O5OelGKokv"
   },
   "source": [
    "### Load Latest Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "710QCwSJq78O",
    "outputId": "de86a2b0-ee11-4d01-95a8-486a7e032248"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/training_checkpoints\\\\ckpt_100'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch the latest checkpoint from the model directory\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "C33Lo5A4q7yQ"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "VM5cCjyeq7ZP",
    "outputId": "7ac46319-ca4f-4df4-e953-8ed06debf54b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (1, None, 256)            16128     \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (1, None, 1024)           5246976   \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (1, None, 1024)           8392704   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (1, None, 63)             64575     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,720,383\n",
      "Trainable params: 13,720,383\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_J0tN6RK43x"
   },
   "source": [
    "### Utility Function to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Zge9K2AIO3TF"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, mode='greedy', context_string='Hello', num_generate=1000, \n",
    "                  temperature=1.0):\n",
    "    \"\"\"\n",
    "    Utility to generate text given a trained model and context\n",
    "    Parameters:\n",
    "        model: tf.keras object trained on a sufficiently sized corpus\n",
    "        mode: decoding mode. Default is greedy. Other mode is\n",
    "              sampling (set temperature)\n",
    "        context_string: input string which acts as context for the model\n",
    "        num_generate: number of characters to be generated\n",
    "        temperature: parameter to control randomness of outputs\n",
    "    Returns:\n",
    "        string : context_string+text_generated\n",
    "    \"\"\"\n",
    "\n",
    "    # vectorizing: convert context string into string indices\n",
    "    input_eval = [char2idx[s] for s in context_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # String for generated characters\n",
    "    text_generated = []\n",
    "    beam_input_predictions = []\n",
    "    model.reset_states()\n",
    "    # Loop till required number of characters are generated\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        if mode == 'greedy':\n",
    "          predicted_id = np.argmax(predictions[0])\n",
    "          \n",
    "        elif mode == 'sampling':\n",
    "          # temperature helps control the character returned by the model.\n",
    "          predictions = predictions / temperature\n",
    "          # Sampling over a categorical distribution\n",
    "          predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # predicted character acts as input for next step\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return (context_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSviXqdnK-gW"
   },
   "source": [
    "### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "JP3bh8OorG9c",
    "outputId": "46f65d47-ad4b-476a-f438-dfe6c27ae4ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assalamu'alaikummu, setira kita bersama. Tapi hati ini merasa sendiri ketika kau tak di sisikuâ€¦aku begitu merindukanmu.\n",
      "\n",
      "Kusadari, mungkin ini yang disebut cinta. Yah . . .ku beranikan diri ku katakan bahwa aku mencintaimu, sangat mencintaimu.\n",
      "\n",
      "Terserah apa yang ada di benakmu saat ini tentangku. Namun, aku berharap balasan surat darimu yang akan menyejukkan hatiku.\n",
      "\n",
      "Yang mencintaimu,\n",
      "\n",
      "Aku tidak mengerti apa itu cinta dan bagaimana rasa sayang bisa terjadi.\n",
      "\n",
      "Namun, kau hadir mengubah semuanya menjadi lebih indah. Tatkala hati ini beku, kaulah yang telah mencairkannya dengan penuh kelembutan layaknya anak panah yang menembus palung terdalam.\n",
      "\n",
      "Kadang-kadang, terbersit dalam pikiran bahwa cinta itu aneh. Ada yang mengatakan kalau cinta itu tidak memiliki kaki, tapi cinta dapat berjalan dari hati ke hati, dan kini cintaku tlah berjalan ke hatimu. Kini cinta ini tlah lumpuh untuk tetap berada di hatimu, tidak akan pernah sanggup untuk berjalan meninggalkanmu.\n",
      "\n",
      "Kasih. . .cintaku itu sederhana.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'greedy', context_string=\"Assalamu'alaikum\",num_generate=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbFJp-h8LF6E"
   },
   "source": [
    "### Sampled Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "feWUSIl0dMlu",
    "outputId": "fec6f4d0-f96e-422f-9ce7-7605bf182adc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sayang kuâ€¦\n",
      "\n",
      "Sayangku, lebih baik bagiku berjalan bersamamu menyusuri pematang di pesawahan di lereng bukit di sebelah Barat desamu daripada memandangi kota Paris dari atas Menara Eiffel. Karena di pesawahan itu, aku dapat menggennggam jemarimu dan membaui aroma lembut rambutmu. Sedangkan di Eiffel, aku hanya dapat membayangkan wajahmu yang selalu menyejukkan hatiku.\n",
      "\n",
      "Sayangku, terasa lebih nikmat bagiku, saat menyantap maso malang semangkuk berdua dengan tempias hujan sekali-kali mengelus muka kita, daripada makan di restoran mewah di Paris bersama bos. Di baso malang aku puas mengumpulkan senyummu sementara di Paris aku selalu teringat padamu.\n",
      "\n",
      "Sayangku, waktu serasa berhenti apabila kuhitung hari ku kan kembali. Lama sekali rasanya. Kupikir bukan lagi sehari rasa seminggu tetapi sehari rasanya berabad-abad yang harus kulewati.\n",
      "\n",
      "Sayangku, kuharap Engkau sudi sabar menunggu. Tiga bulan lagi aku akan selesai dengan tugasku di sini. Saat hari tiu tiba nanti, aku akan langsung menemuimu. A\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'sampling', context_string=u\"sayang ku\",num_generate=1000,temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "KeEEU42VcNkH",
    "outputId": "f6f9c03e-d84a-4052-ad3c-6f79ba3b933a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rindu.\n",
      "\n",
      "Sejak pertama kali aku mengenalmu, tak ada yang berbeda dari rasa ini. Tak ada yang berbeda dar\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'sampling', context_string=u\"rindu\",num_generate=100,temperature=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "iOI_Yh5-WS9t",
    "outputId": "64fc4f52-3284-478e-a5a3-7159d6d9a52f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malam surat ini sebagai bentuk rasa rinduku padamu. Surat ini aku buat ketika cuaca dan pagakan kita basa\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'sampling', context_string=u\"malam\",num_generate=100,temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDPPRnG-PRAn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "language_model_stacked_lstm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
